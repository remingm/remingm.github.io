<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>TensorFlow Learn Tutorial - Google&#39;s Simple Machine Learning Framework</title>
  <meta name="description" content="If you are new to machine learning, I recommend first watching these videos made by Google.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://remingm.github.io/machine/learning/tensorflow/neural/networks/2016/06/24/tflearn-tutorial.html">
  <link rel="alternate" type="application/rss+xml" title="Michael Remington" href="http://remingm.github.io/feed.xml">



</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Michael Remington</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>


      <div class="trigger">
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">TensorFlow Learn Tutorial - Google's Simple Machine Learning Framework</h1>
    <p class="post-meta"><time datetime="2016-06-24T19:58:49-07:00" itemprop="datePublished">Jun 24, 2016</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>If you are new to machine learning, I recommend first watching <a href="https://youtu.be/cKxRvEZd3Mw?list=PLT6elRN3Aer7ncFlaCz8Zz-4B5cnsrOMt">these videos made by Google.</a></p>

<p>In this tutorial, we’ll</p>

<ol>
  <li>Load our data into pandas dataframes</li>
  <li>Convert categorical text data into one-hot vectors and numerical vectors</li>
  <li>Normalize continuous features</li>
  <li>Split our data into random train and dev sets</li>
  <li>Run a deep neural network</li>
  <li>Make a custom model with batch normalization</li>
  <li>Tune the model with random sampling</li>
  <li>Log our hyper parameter search to a sortable csv file</li>
</ol>

<p>For more tutorials and examples, see the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">TF Learn home page</a>.
In this tutorial, all code should be written to one file.</p>

<p>The full code for this tutorial can be found <a href="https://github.com/remingm/TF-Learn-Tutorial/blob/master/TFLearn_Tutorial.py">here</a>.</p>

<h2 id="part-0--get-packages-and-data">Part 0 : Get Packages and Data</h2>

<p>This tutorial uses the common iris dataset.
<a href="https://github.com/remingm/TF-Learn-Tutorial/archive/master.zip">Click here to download the data for this tutorial.</a></p>

<p>You will need to install TensorFlow, pandas, and sklearn. Installation instructions can be found at these project’s websites. TF Learn is part of TensorFlow. NOTE: This tutorial uses TensorFlow 0.8. As of this writing, TensorFlow 0.9 has <a href="https://github.com/tensorflow/tensorflow/issues/2727">bugs that can complicate using simple models</a>.</p>

<h1 id="tensorflow-08-installation">TensorFlow 0.8 Installation:</h1>
<p>Install pip (or pip3 for python3) if it is not already installed:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># Ubuntu/Linux 64-bit</span>
<span class="gp">$ </span>sudo apt-get install python-pip python-dev

<span class="c"># Mac OS X</span>
<span class="gp">$ </span>sudo easy_install pip
</code></pre>
</div>

<p>Install TensorFlow:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># Ubuntu/Linux 64-bit, CPU only, Python 2.7:</span>
<span class="gp">$ </span>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl

<span class="c"># Ubuntu/Linux 64-bit, GPU enabled, Python 2.7. Requires CUDA toolkit 7.5 and cuDNN v4.</span>
<span class="c"># For other versions, see "Install from sources" below.</span>
<span class="gp">$ </span>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl

<span class="c"># Mac OS X, CPU only:</span>
<span class="gp">$ </span>sudo easy_install --upgrade six
<span class="gp">$ </span>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl
</code></pre>
</div>

<p>For python3:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># Ubuntu/Linux 64-bit, CPU only, Python 3.4:</span>
<span class="gp">$ </span>sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl

<span class="c"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.4. Requires CUDA toolkit 7.5 and cuDNN v4.</span>
<span class="c"># For other versions, see "Install from sources" below.</span>
<span class="gp">$ </span>sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl

<span class="c"># Mac OS X, CPU only:</span>
<span class="gp">$ </span>sudo easy_install --upgrade six
<span class="gp">$ </span>sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py3-none-any.whl
</code></pre>
</div>

<h2 id="part-1--load-data">Part 1 : Load Data</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># This tutorial will use TensorFlow, TF Learn, sklearn, and pandas</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.contrib.skflow</span> <span class="kn">as</span> <span class="nn">learn</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">import</span> <span class="nn">pandas</span>
</code></pre>
</div>

<p>First we’ll load our raw data and labels into pandas dataframes</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'iris.data'</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'iris.labels'</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c"># Ignore this. For later one-hot example.</span>
<span class="n">text_labels</span> <span class="o">=</span> <span class="n">labels</span>
</code></pre>
</div>

<p>Let’s take a look at our data shapes</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre>
</div>

<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">(</span>150, 4<span class="o">)</span>
<span class="o">(</span>150, 1<span class="o">)</span>
</code></pre>
</div>

<p>And our raw data</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre>
</div>

<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>0    1    2    3   
0  5.1  3.5  1.4  0.2
1  4.9  3.0  1.4  0.2
2  4.7  3.2  1.3  0.2
3  4.6  3.1  1.5  0.2
4  5.0  3.6  1.4  0.2
	  0   
0  Iris-setosa
1  Iris-setosa
2  Iris-setosa
3  Iris-setosa
4  Iris-setosa
</code></pre>
</div>

<p>You can see that each input contains four flower measurements, and each output (label) is a class of the iris plant.</p>

<p>Our model needs to know the number of classes we’re predicting</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">y_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre>
</div>

<h2 id="part-2--categorical-data-processing">Part 2 : Categorical Data Processing</h2>

<h1 id="representing-categories-with-integers">Representing categories with integers</h1>

<p>Our labels are in text format and must be converted to integer values</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">categorical_processor</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">CategoricalProcessor</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">categorical_processor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
</code></pre>
</div>

<p>Let’s look at our new labels. Each category is now a number</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre>
</div>

<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>0   
0  1
1  1
2  1
3  1
4  1
</code></pre>
</div>

<h1 id="representing-categories-with-one-hot-vectors">Representing categories with “one-hot” vectors</h1>

<p>Another way to handle categorical text or numbers is converting to categorical one-hot vectors</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">labels_oneHot</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">text_labels</span><span class="p">)</span>
</code></pre>
</div>
<p>Let’s look at our new one-hot labels. Each column now represents a category. A “one-hot” vector is a vector of zeros with a one in a different position for each category.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">labels_oneHot</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre>
</div>
<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>0_Iris-setosa  0_Iris-versicolor  0_Iris-virginica
0            1.0                0.0               0.0
1            1.0                0.0               0.0
2            1.0                0.0               0.0
3            1.0                0.0               0.0
4            1.0                0.0               0.0
</code></pre>
</div>

<p>We won’t use our one-hot labels for this experiment, but this is a common machine learning task</p>

<h2 id="part-3--normalize-continuous-features">Part 3 : Normalize Continuous Features</h2>

<p>It’s advantageous to scale continuous features (like our flower measurements) to 0 mean and unit standard deviation.
Note: Don’t scale categorical integers.</p>

<p>Let’s look at our unscaled data. You’ll see that the mean and standard deviation are not 0 and 1 for any columns.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</code></pre>
</div>
<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>0           1           2           3   
count  150.000000  150.000000  150.000000  150.000000
mean     5.843333    3.054000    3.758667    1.198667
std      0.828066    0.433594    1.764420    0.763161
min      4.300000    2.000000    1.000000    0.100000
<span class="gp">25%      </span>5.100000    2.800000    1.600000    0.300000
<span class="gp">50%      </span>5.800000    3.000000    4.350000    1.300000
<span class="gp">75%      </span>6.400000    3.300000    5.100000    1.800000
max      7.900000    4.400000    6.900000    2.500000
</code></pre>
</div>

<p>First we’ll create a scaler</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span>
</code></pre>
</div>
<p>Now we’ll scale (normalize) our data.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre>
</div>

<p>Let’s look at our scaled data. Now the mean and standard deviation are (roughly) 0 and 1 for each column.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</code></pre>
</div>
<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>0             1             2             3   
count  1.500000e+02  1.500000e+02  1.500000e+02  1.500000e+02
mean  -4.736952e-16 -6.631732e-16  3.315866e-16 -2.842171e-16
std    1.003350e+00  1.003350e+00  1.003350e+00  1.003350e+00
min   -1.870024e+00 -2.438987e+00 -1.568735e+00 -1.444450e+00
<span class="gp">25%   </span>-9.006812e-01 -5.877635e-01 -1.227541e+00 -1.181504e+00
<span class="gp">50%   </span>-5.250608e-02 -1.249576e-01  3.362659e-01  1.332259e-01
<span class="gp">75%    </span>6.745011e-01  5.692513e-01  7.627586e-01  7.905908e-01
max    2.492019e+00  3.114684e+00  1.786341e+00  1.710902e+00
</code></pre>
</div>

<p>Now we’ll split our data into randomly shuffled train, and dev sets.
Setting a random seed allows for continuity between runs</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre>
</div>

<h2 id="part-4--simple-deep-neural-network">Part 4 : Simple Deep Neural Network</h2>
<p>Now we’ll create a simple deep neural network Tensorflow graph.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">classifier</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">TensorFlowDNNClassifier</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">n_classes</span><span class="o">=</span><span class="n">y_classes</span><span class="p">,</span>
							<span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"Adam"</span><span class="p">,</span>
							<span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</code></pre>
</div>

<p>Here we’ll train our DNN</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">logdir</span><span class="o">=</span><span class="s">'dnnLogs'</span><span class="p">)</span>
</code></pre>
</div>

<p>Output:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Step #100, epoch #25, avg. train loss: 1.23663
</code></pre>
</div>

<p>and evaluate it on our dev data</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_dev</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_dev</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>
</code></pre>
</div>
<p>Output:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>Accuracy: 0.966667
</code></pre>
</div>

<p>Our deep neural network predicted correctly on about 96% of the development set!</p>

<h2 id="part-5--tensorboard">Part 5 : TensorBoard</h2>
<p>Running classifier.fit created the directory ‘dnnLogs’.
Comment out the following line in part 3 above:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre>
</div>
<p>Run the DNN again. It will run without scaled features.</p>

<p>Run the following command in a shell from the working directory: <code class="highlighter-rouge">tensorboard --logdir dnnLogs</code></p>

<p>Then go to <code class="highlighter-rouge">localhost:6006</code> in your browser.
Click “Histograms”, and then click ‘X’.
You’ll see two graphs of X. Notice that one is centered around zero and the other isn’t. This is what scaling does.</p>

<p>Click “Graph” at the top to see a visual representation of the DNN TensorFlow graph we created.</p>

<p>Uncomment the line you commented out before so the features will be scaled again.</p>

<h2 id="part-6--custom-model">Part 6 : Custom Model</h2>

<p>Next, we’ll define a custom classifier model. You can use any TensorFlow or TF Learn code in this function.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">custom_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>

    <span class="c"># When running, X is a tensor of [batch size, num feats] and y is a tensor of [batch size, num outputs]</span>

    <span class="c"># This model will use a technique called batch normalization</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">batch_normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">scale_after_normalization</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c"># Now we'll pass our normalized batch to a DNN</span>
    <span class="c"># We can pass a TensorFlow object as the activation function</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">dnn</span><span class="p">(</span><span class="n">X</span><span class="p">,[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span><span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="c"># Given encoding of DNN, take encoding of last step (e.g hidden state of the</span>
    <span class="c"># neural network at the last step) and pass it as features for logistic</span>
    <span class="c"># regression over the label classes.</span>
    <span class="k">return</span> <span class="n">learn</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">logistic_regression</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre>
</div>

<p>We need a generic TF Learn model to wrap our custom model.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">classifier</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">TensorFlowEstimator</span><span class="p">(</span><span class="n">model_fn</span><span class="o">=</span><span class="n">custom_model</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="n">y_classes</span><span class="p">,</span>
						   <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                                       <span class="n">optimizer</span><span class="o">=</span><span class="s">"Adam"</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</code></pre>
</div>

<p>We’ll make a function for training and evaluating</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_model</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span><span class="n">logdir</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">monitor</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c"># Train</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">logdir</span><span class="o">=</span><span class="n">logdir</span><span class="p">,</span><span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">)</span>

    <span class="c"># Evaluate on dev data</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_dev</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_dev</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">run_model</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span><span class="s">'customModelLogs'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>
</code></pre>
</div>
<p>Output:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Step #100, epoch #25, avg. train loss: 0.72944
Step #200, epoch #50, avg. train loss: 0.48746
Step #300, epoch #75, avg. train loss: 0.41292
Step #400, epoch #100, avg. train loss: 0.38321
Step #500, epoch #125, avg. train loss: 0.33969
Accuracy: 1.000000
</code></pre>
</div>

<p>We got 100% accuracy, but keep in mind our dataset has only 150 datapoints.</p>

<h2 id="part-7--hyperparameter-search">Part 7 : Hyperparameter Search</h2>
<p>Random sampling is a way to search for the optimal parameters for a model.
It’s recommended in ‘Bengio. Practical Recommendations for Gradient-Based Training of Deep Architectures. 2012.’</p>

<p>We’ll make a function that will randomly generate hyper parameters or return set ones.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">getHyperparameters</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">tune</span><span class="p">:</span>

        <span class="c"># Randomize DNN layers and hidden size</span>
        <span class="n">hidden_units</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">NUNITS</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">NLAYERS</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">NLAYERS</span><span class="p">):</span>
            <span class="n">hidden_units</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NUNITS</span><span class="p">)</span>

        <span class="c"># Make dict of randomized hyper params</span>
        <span class="n">hyperparams</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'BATCH_SIZE'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
        <span class="s">'STEPS'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
        <span class="s">'LEARNING_RATE'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">),</span>
        <span class="s">'OPTIMIZER'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s">"SGD"</span><span class="p">,</span> <span class="s">"Adam"</span><span class="p">,</span> <span class="s">"Adagrad"</span><span class="p">]),</span>
        <span class="s">'HIDDEN_UNITS'</span><span class="p">:</span><span class="n">hidden_units</span><span class="p">,</span>
        <span class="s">'NUM_LAYERS'</span><span class="p">:</span><span class="n">NLAYERS</span><span class="p">,</span>
        <span class="s">'NUM_UNITS'</span><span class="p">:</span><span class="n">NUNITS</span><span class="p">,</span>
        <span class="s">'ACTIVATION_FUNCTION'</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">]),</span>
        <span class="s">'KEEP_PROB'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s">'MAX_BAD_COUNT'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>

        <span class="n">hyperparams</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'BATCH_SIZE'</span><span class="p">:</span><span class="mi">32</span><span class="p">,</span>
        <span class="s">'STEPS'</span><span class="p">:</span><span class="mi">1000</span><span class="p">,</span>
        <span class="s">'LEARNING_RATE'</span><span class="p">:</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="s">'OPTIMIZER'</span><span class="p">:</span><span class="s">"Adam"</span><span class="p">,</span>
        <span class="s">'HIDDEN_UNITS'</span><span class="p">:</span><span class="n">hidden_units</span><span class="p">,</span>
        <span class="s">'NUM_LAYERS'</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">),</span>
        <span class="s">'NUM_UNITS'</span><span class="p">:</span><span class="n">hidden_units</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s">'ACTIVATION_FUNCTION'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="s">'KEEP_PROB'</span><span class="p">:</span><span class="mf">0.6</span><span class="p">,</span>
        <span class="s">'MAX_BAD_COUNT'</span><span class="p">:</span><span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">return</span> <span class="n">hyperparams</span>
</code></pre>
</div>

<p>Next we’ll wrap our model in a function so that we can repeatedly instantiate it with new hyper-parameters.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">instantiateModel</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">):</span>

    <span class="c"># We'll copy the same model from above</span>
    <span class="k">def</span> <span class="nf">custom_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">batch_normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">scale_after_normalization</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">dnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s">'HIDDEN_UNITS'</span><span class="p">],</span>
	  					<span class="n">activation</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'ACTIVATION_FUNCTION'</span><span class="p">],</span>
						<span class="n">dropout</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'KEEP_PROB'</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">learn</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">logistic_regression</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">classifier</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">TensorFlowEstimator</span><span class="p">(</span><span class="n">model_fn</span><span class="o">=</span><span class="n">custom_model</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="n">y_classes</span><span class="p">,</span>
						<span class="n">batch_size</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'BATCH_SIZE'</span><span class="p">],</span>
						<span class="n">steps</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'STEPS'</span><span class="p">],</span><span class="n">optimizer</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'OPTIMIZER'</span><span class="p">],</span>
						<span class="n">learning_rate</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'LEARNING_RATE'</span><span class="p">])</span>

    <span class="c"># We'll make a monitor so that we can implement early stopping based on our train accuracy. This will prevent overfitting.</span>
    <span class="n">monitor</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">monitors</span><span class="o">.</span><span class="n">BaseMonitor</span><span class="p">(</span><span class="n">early_stopping_rounds</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">[</span><span class="s">'MAX_BAD_COUNT'</span><span class="p">]),</span><span class="n">print_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">monitor</span>
</code></pre>
</div>

<p>Now we’ll ‘tune’ our model by running a hyper parameter search over many runs</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span> <span class="c"># Raise this number for a more thorough hyper-parameter search</span>

    <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">getHyperparameters</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)</span>
    <span class="n">classifier</span><span class="p">,</span><span class="n">monitor</span> <span class="o">=</span> <span class="n">instantiateModel</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">run_model</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span><span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>

    <span class="c"># We don't need to log this array</span>
    <span class="k">del</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s">'HIDDEN_UNITS'</span><span class="p">]</span>

    <span class="c"># Now we'll add the dev set accuracy to our dict</span>
    <span class="n">hyperparams</span><span class="p">[</span><span class="s">'dev_Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>

    <span class="c"># Convert the dict to a dataframe</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>

    <span class="c"># Write to a csv file</span>
    <span class="n">csvName</span> <span class="o">=</span> <span class="s">'model_log.csv'</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">csvName</span><span class="p">)):</span>
        <span class="c"># First run, write headers</span>
        <span class="n">log</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'model_log.csv'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'a'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'model_log.csv'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'a'</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre>
</div>

<p>Open the csv file in libreoffice or a similar editor. Now you can sort thousands of runs by dev accuracy and find the best hyperparameters.</p>

<p>Tuning is important. If you check model_log.csv you’ll see that the same model can do poorly or excellently with different hyperparameters.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Michael Remington</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Michael Remington</li>
          <li><a href="mailto:michael.remington@outlook.com">michael.remington@outlook.com</a></li>
          <li><a href="/pdfs/Remington_Resume.pdf">Resume</a></li>
          <li><a href="https://www.linkedin.com/in/remingtonmichael">LinkedIn</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>
	<a data-sumome-discover-grid></a>

  </div>


	<script src="//load.sumome.com/" data-sumo-site-id="c7ab691945de36b5d36edfa0ce7adfb1a53521700bd66ceaf87ccb252daf43b8" async="async"></script>


	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-83996371-1', 'auto');
	  ga('send', 'pageview');

	</script>

</footer>


  </body>

</html>
